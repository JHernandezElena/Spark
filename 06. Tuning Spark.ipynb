{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"> Tuning Spark </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/book_tuning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque Apache Spark mejora cada día conseguir que un proceso de Spark funcione correctamente puede ser complicado en algunos escenarios: Muchos datos, operaciones muy costosas... \n",
    "\n",
    "Muchos de ellos acaban con el famosísimo error `Spark java.lang.StackOverflowError` que para los que vienen del mundo JVM les será bien conocido y da nombre a la famosa web https://stackoverflow.com/.\n",
    "\n",
    "\n",
    "En esta sección vamos a ver algunos consejos de cómo configurar nuestra aplicación de spark para conseguir una buena *performance* y evitar los errores más típicos, pero ya hay que adelantar que no hay una fórmula mágica. Dependerá mucho de nuestro cluster, nuestro proceso así que cada proceso es distinto y hay que ir probando cada una de las técnicas hasta conseguir que nuestro proceso sea estable y rápido.\n",
    "\n",
    "\n",
    "Existen varios recursos en internet sobre estos temas, nos hemos basados en los siguientes:\n",
    "\n",
    "\n",
    "* https://spark.apache.org/docs/latest/tuning.html\n",
    "\n",
    "* https://databricks.com/training/instructor-led-training/courses/apache-spark-tuning-and-best-practices\n",
    "\n",
    "* https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/\n",
    "\n",
    "* https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/\n",
    "\n",
    "\n",
    "\n",
    "Como hemos comentado, cada caso es un mundo, en esta pequeña guía nos vamos a centrar en **Spark on YARN**, pudiendo ser distinto la configuración si usamos otro manager (*standalone*, Mesos, Kubernetes...)\n",
    "\n",
    "\n",
    "![](img/spark-yarn-cluster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;   \n",
    "\n",
    "\n",
    "## Configuración estándar y estudio de nuestro cluster\n",
    "\n",
    "Lo primero que tenemos que ver es el tamaño de nuestro cluster (cores totales y RAM total), así cómo saber si el proceso que vamos a ejecutar va a estar solo en toda la máquina o vamos a tener que compartir con otros usuarios al mismo tiempo.\n",
    "\n",
    "\n",
    "Podemos acceder al Hadoop Task Manager para ver las características de nuestro cluster:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](img/tamano_cluster.png)\n",
    "<center>\n",
    "http://master02.bigdata.alumnos.upcont.es:8088/cluster/nodes\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De aquí podemos extraer que nuestro cluster tiene 400GB de memoria RAM y 256 cores. Además esta RAM y cores está dividida de manera balanceada en 4 nodos (100GB y 64cores).\n",
    "\n",
    "Si nuestra aplicación se fuese a correr sola en todo el cluster podríamos configurar los executors como:\n",
    "\n",
    "* `spark.executor.memory` = 100g\n",
    "* `spark.executor.cores` = 64\n",
    "\n",
    "Pero esta configuración no suele ser la adecuada ya que suele ser mayor que lo que permite YARN en la variable `yarn.scheduler.maximum-allocation-mb` además según estudios realizados si vamos a trabajar en YARN y con HDFS no se suele recomendar usar más de 5 cores por JVM ([ HDFS I/O throughput](https://www.cloudera.com/documentation/enterprise/5-13-x/topics/admin_spark_tuning.html)).\n",
    "\n",
    "Además del problema del I/O con HDFS suele ser bueno hacer executors más pequeños para que el proceso pueda convivir con otros usuarios. Siguiendo la idea de intentar sacar el mayor provecho a los recursos vamos a calcular el ratio RAM(Gb)/cores de nuestro cluster:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{400}{256} \\approx 1.6\n",
    "$$\n",
    "\n",
    "Así que si fijamos los cores a 5 e intentamos mantener esta proporción, podemos calcular que el número de GB adecuado sería 8.\n",
    "\n",
    "Para terminar hay que tener en cuenta otra configuración `spark.executor.memoryOverhead`:\n",
    "\n",
    "\n",
    "![](https://ndu0e1pobsf1dobtvj5nls3q-wpengine.netdna-ssl.com/wp-content/uploads/2019/08/spark-tuning2-f1.png)\n",
    "\n",
    "\n",
    "Además de la memoria que reservamos para cada executor tenemos que tener en cuenta también la configuración del *overhead* que es un trozo de memoria que se deja reservado y que por defecto:\n",
    "\n",
    "![](img/overhead.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que, si usamos el valor por defecto tendríamos que tener en cuenta ese 10% en nuestra cuenta:\n",
    "\n",
    "$$\n",
    "RAM = \\frac{1.6 \\cdot 5}{1.1} \\approx  7.27\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De este modo llegamos a la configuración:\n",
    "\n",
    "* `spark.executor.memory` = 7g\n",
    "* `spark.executor.cores` = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "\n",
    "    SparkConf()\n",
    "    .setAppName(u\"[ICAI] Spark Tuning\")\n",
    "    .set(\"spark.executor.memory\", \"7g\") \n",
    "    .set(\"spark.executor.cores\", \"5\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "\n",
    "    SparkSession.builder\n",
    "    .config(conf=conf)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a forzar cargar varios executors para ver la distribución en el cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = spark.table('jayuso.ratings').repartition(1000).sample(True, 100.0).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1534180093"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consiguiendo:\n",
    "\n",
    "\n",
    "![](img/proceso_distribuido.png)\n",
    "\n",
    "&nbsp;   \n",
    "\n",
    "\n",
    "Que si hacemos los calculos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5975103734439835"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "394240 / 1024 / 241"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_contenido: double, id_user: double, franja: string, ratings: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe vs RDD:  Tungsten y Catalyst\n",
    "\n",
    "Como ya hemos visto la API DataFrame en general es más rápida y eficiente que trabajar directamente con RDD's. Esto es en gran medida a los proyectos Tungsten y Catalyst.\n",
    "\n",
    "Veamos como los mismos datos ocupan mucho menos en formato DataFrame (que usa el proyecto Tungsten para serializar los datos) que en formato RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = spark.table('jayuso.ratings').cache() #aqui comprime los datos en parquet y es muu compromodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15342422"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta2 = meta.rdd.cache() #esto es mucho mas grnade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15342422"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/df_rdd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos nuestros datos en formato DataFrame podemos usar todas las funciones que ya hemos visto (las `F.*`). Estas funciones saben trabajar directamente en el formato de Tungsten y así ser más eficiente.\n",
    "\n",
    "\n",
    "Si tenemos que usar una o más UDF de python/scala puro lo mejor sería hacerlas todas a la vez y así no tener que serializar/deserializar varias veces:\n",
    "\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/tuning/interleaved-lambdas.png\" alt=\"Interleaved Lambdas\" style=\"border: 1px solid #cccccc; margin: 20px\"/>\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/tuning/chained-lambdas.png\" alt=\"Chained Lambdas\" style=\"border: 1px solid #cccccc; margin: 20px\"/><br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Conclusión:**  Siempre que podamos utilizar DataFrame y utilizar las funciones propias de Spark. Si tenemos que usar UDF hacerlas si se puede juntas y cuando los datos sean lo más pequeños posibles (después de hacer los filtros y seleccionar las columnas estrictamente necesarias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[30] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.unpersist()\n",
    "meta2.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling (¿barajando?)\n",
    "\n",
    "Dentro de las transformaciones que hacemos en Spark podemos distinguir dos tipos:\n",
    "\n",
    "#### Transformaciones Wide vs Narrow\n",
    "\n",
    "#### Transformaciones Narrow\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/transformations-narrow.png\" alt=\"Narrow Transformations\" style=\"height: 300px\"/>\n",
    "\n",
    "Las transformaciones de tipo Narrow pueden hacerse todas a la vez en un único *stage*. (FILTER, SELECT, WITHCOLUMN)\n",
    "\n",
    "\n",
    "#### Transformaciones Wide\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/transformations-wide.png\" alt=\"Wide Transformations\" style=\"height: 300px\"/>\n",
    "\n",
    "- Las transformaciones Wide causan *shuffling* y esto produce varios *stages*\n",
    "- Algunas de estas transformaciones son: `distinct`, `join`, `orderBy`, `groupBy`.\n",
    "SIMEPRE QUE HAGAMOS ESTAS FUNCIONES VA A SER MAS COSTOSO\n",
    "Primero hacer selec, filters etc para conseguir un dataset mas pequeño\n",
    "\n",
    "&nbsp;    \n",
    "\n",
    "\n",
    "Veamos algún ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todas estas transformaciones son narrow\n",
    "meta_narrow = (\n",
    "\n",
    "    spark.read.load('/datos/reviews_amazon.parquet')\n",
    "    .withColumn('md5_col', F.md5('review'))\n",
    "    .withColumn('dummy', F.lit(100))\n",
    "    .withColumn('nuevo', F.col('rating') + F.col('dummy'))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19959"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_narrow.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/narrow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con transformaciones wide\n",
    "meta_wide = (\n",
    "\n",
    "    spark.read.load('/datos/reviews_amazon.parquet')\n",
    "    .withColumn('md5_col', F.md5('review'))\n",
    "    .withColumn('md5_col_subt', F.substring('md5_col', 0, 1))\n",
    "    .groupBy('md5_col_subt')\n",
    "    .count()\n",
    "    .withColumn('nuevo', F.col('count') + F.rand())\n",
    "    .orderBy(F.desc('nuevo'))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_wide.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/wide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las operaciones de tipo *wide* son mucho más costosas, así que hay que hacerlas cuando sean necesarias y siempre intentar conseguir el DataFrame más pequeño posible (filtrar y seleccionar las columnas necesarias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nivel de paralelismo\n",
    "\n",
    "El nivel de paralelismo es clave para que un proceso de Spark (sobre todo los que involucran transformaciones *wide*) funcione o no, o tarde unos minutos u horas.\n",
    "\n",
    "Veamos algún ejemplo y entendamos las dos configuraciones importantes, se ha ejecutado (no lo hacemos ahora por tardar mucho):\n",
    "SIN NADA DE SHUFFLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "(\n",
    "    \n",
    "    spark.table(\"jayuso.huge_table_v2\")\n",
    "    .withColumn('palabras', F.split('review', '\\s+'))\n",
    "    .withColumn('palabras', F.explode('palabras'))\n",
    "\n",
    ").write.mode('overwrite').saveAsTable(\"jayuso.huge_table\")\n",
    "```\n",
    "\n",
    "El proceso contiene multitud de errores como este:\n",
    "\n",
    "\n",
    "```\n",
    "ExecutorLostFailure (executor 15 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 8.0 GB of 8 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
    "```\n",
    "\n",
    "y tarda mucho:\n",
    "\n",
    "\n",
    "![](img/muy_lento.png)\n",
    " \n",
    " \n",
    "&nbsp;   \n",
    "\n",
    "> (1) **¿Qué significa el 399?**\n",
    "Son las particiones que hace en paralelo\n",
    "\n",
    "\n",
    "&nbsp;   \n",
    "\n",
    "\n",
    "\n",
    "Veamos otro ejemplo: CON SHUFFLES\n",
    "\n",
    "\n",
    "```python\n",
    "(\n",
    "    \n",
    "    spark.table(\"jayuso.huge_table_v2\")\n",
    "    .withColumn('dummy', F.substring('md5_col', 0, 1))\n",
    "    .groupBy('dummy')\n",
    "    .agg(F.collect_list('review').alias('lista'))\n",
    "\n",
    ").write.mode('overwrite').saveAsTable(\"jayuso.huge_table\")\n",
    "```\n",
    "\n",
    "&nbsp;   \n",
    "\n",
    "\n",
    "![](img/error.png)\n",
    "\n",
    "\n",
    "&nbsp;   \n",
    "\n",
    "\n",
    "> (2) **¿Qué significa el 200 del segundo *stage*?**\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Para entender estos números es necesario entender el nivel de paralelismo es decir el número de particiones en el que está dividido nuestro DataFrame en el cluster.\n",
    "\n",
    "\n",
    "\n",
    "### Ejemplo 1\n",
    "\n",
    "Los 399 es un número que calcular spark de manera automática analizando el DafaFrame que va a leer y estima el número de particiones necesario para trabajar con él. Veamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = spark.table(\"jayuso.huge_table_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.rdd.getNumPartitions() #numero de particiones por defecto que consigue spark haciendo un analisis rapido cerca de 399\n",
    "#SE PUEDE MODIFICAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este número se calcula como:\n",
    "\n",
    "\n",
    "\n",
    "máximo(`fileSize` / `maxPartitionSize`, `spark.sparkContext.defaultParallelism`)\n",
    "\n",
    "\n",
    "\n",
    "dónde, `fileSize` es la estimación que hace Spark sobre el DataFrame que va a leer y los otros dos son parámetros de configuración con valores por [defecto](https://spark.apache.org/docs/latest/configuration.html).\n",
    "\n",
    "En general este valor que se calcula de manera automática está bien para casi todos los usos, aunque si vamos a tener que generar muchos datos más como en el ejemplo 1 con `F.explode` nos interesa que el nivel de paralelismo sea mayor. Esto lo podemos hacer con `repartition` o mejor configuración con un valor a mano la variable `spark.sparkContext.defaultParallelism`.\n",
    "\n",
    "\n",
    "&nbsp;   \n",
    "\n",
    "\n",
    "### Ejemplo 2 (LO MAS IMPORTANTE DEL NOTEBOOK!!!! - entra en el exm)\n",
    "\n",
    "En el segundo ejemplo, hemos realizado una transformación de tipo *wide* (`groupBy`) el primer stage de lectura tienen 399 particiones, el segundo tienen 200. El motivo de este 200 es un parámetro muy importante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.shuffle.partitions') #=el nivel de paralelismo que usara para una transf tipo WIDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto Spark siempre hará las operaciones shuffle con 200 particiones, este número **no depende de la magnitud de nuestros datos**, así que es muy importante tenerlo en cuenta e ir variándolo.\n",
    "\n",
    "\n",
    "Vamos a iniciar una nueva sesión de spark cambiando estos parámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "\n",
    "    SparkConf()\n",
    "    .setAppName(u\"[ICAI] Spark Tuning\")\n",
    "    .set(\"spark.executor.memory\", \"7g\")\n",
    "    .set(\"spark.executor.cores\", \"5\")\n",
    "    .set(\"spark.default.parallelism\", 3000) #DA IGUAL EL DATA SET QUE VAYA A LEER, LO VA A TRABAJAR CON 3000 PARTICIONES\n",
    "            #para la formula de la particion de un RDD max(.., ..)\n",
    "    .set(\"spark.sql.shuffle.partitions\", 3000) #PARA FUNCIONES SHUFFLE \n",
    "\n",
    ")\n",
    "#en el master con 200 nos va a valer siempre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "\n",
    "    SparkSession.builder\n",
    "    .config(conf=conf)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = spark.table(\"jayuso.huge_table_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.rdd.getNumPartitions() #puede ser cercano a 3000 no exactamene 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces podríamos lanzar de nuevo los comandos y funcionarán (no lo hacemos que tarda mucho):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "(\n",
    "\n",
    "    spark.table(\"jayuso.huge_table_v2\")\n",
    "    .withColumn('palabras', F.split('review', '\\s+'))\n",
    "    .withColumn('palabras', F.explode('palabras'))\n",
    "\n",
    ").write.mode('overwrite').saveAsTable(\"jayuso.huge_table\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "(\n",
    "\n",
    "    spark.table(\"jayuso.huge_table_v2\")\n",
    "    .withColumn('dummy', F.substring('md5_col', 0, 1))\n",
    "    .groupBy('dummy')\n",
    "    .agg(F.collect_list('review').alias('lista'))\n",
    "\n",
    ").write.mode('overwrite').saveAsTable(\"jayuso.huge_table\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcast Join\n",
    "\n",
    "&nbsp;   \n",
    "\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/tuning/broadcast-join.png\" style=\"height:300px;\"  alt=\"Spill to disk\"/><br/><br/>    \n",
    "\n",
    "Este tipo de *join* es muy importante cuando tenemos un DataFrame grande y otro pequeño veamos un ejemplo:\n",
    "PASA LA TABLA PEQUENA A TODAS LAS MAQUINAS PARA NO TENER QUE HACER PRIMERO UN SORT, MERGE, JOIN (ordenar por el campo join y mandar cachos ordenados a las mismas maquinas = primer cacho de una tabla y de otra a la misma maquina)\n",
    "ASI LA MAQUINA GRANDE NO SE TIENE QUE MOVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiencias = spark.read.parquet('/datos/ejercicio_audis/audiencias.parquet')\n",
    "catalogo = spark.read.json('/datos/ejercicio_audis/info_contenidos.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25595651"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audiencias.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116290"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[duracion: bigint, id_contenido: bigint]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge1 = audiencias.join(catalogo, 'id_contenido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [id_contenido#218, id_user#217, franja#219, segundos_visualizados#220L, duracion#231L]\n",
      "+- *(2) BroadcastHashJoin [id_contenido#218], [cast(id_contenido#232L as double)], Inner, BuildRight\n",
      "   :- *(2) Project [id_user#217, id_contenido#218, franja#219, segundos_visualizados#220L]\n",
      "   :  +- *(2) Filter isnotnull(id_contenido#218)\n",
      "   :     +- *(2) FileScan parquet [id_user#217,id_contenido#218,franja#219,segundos_visualizados#220L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://nameservice1/datos/ejercicio_audis/audiencias.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(id_contenido)], ReadSchema: struct<id_user:double,id_contenido:double,franja:string,segundos_visualizados:bigint>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, bigint, true] as double)))\n",
      "      +- *(1) Project [duracion#231L, id_contenido#232L]\n",
      "         +- *(1) Filter isnotnull(id_contenido#232L)\n",
      "            +- *(1) FileScan json [duracion#231L,id_contenido#232L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://nameservice1/datos/ejercicio_audis/info_contenidos.json], PartitionFilters: [], PushedFilters: [IsNotNull(id_contenido)], ReadSchema: struct<duracion:bigint,id_contenido:bigint>\n"
     ]
    }
   ],
   "source": [
    "merge1.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar como por defecto ha realizado un `BroadcastHashJoin` en la tercera linea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.73 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1.9 s ± 1.26 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit merge1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto spark hará *broadcast join* si detecta que uno de los dos DataFrames involucrados es menor que:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10485760'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.autoBroadcastJoinThreshold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si por ejemplo desactivamos este parámetro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.autoBroadcastJoinThreshold', -1) #DESACTIVAMOS EL BROADCAST JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge2 = audiencias.join(catalogo, 'id_contenido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [id_contenido#218, id_user#217, franja#219, segundos_visualizados#220L, duracion#231L]\n",
      "+- *(5) SortMergeJoin [id_contenido#218], [cast(id_contenido#232L as double)], Inner\n",
      "   :- *(2) Sort [id_contenido#218 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(id_contenido#218, 3000)\n",
      "   :     +- *(1) Project [id_user#217, id_contenido#218, franja#219, segundos_visualizados#220L]\n",
      "   :        +- *(1) Filter isnotnull(id_contenido#218)\n",
      "   :           +- *(1) FileScan parquet [id_user#217,id_contenido#218,franja#219,segundos_visualizados#220L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://nameservice1/datos/ejercicio_audis/audiencias.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(id_contenido)], ReadSchema: struct<id_user:double,id_contenido:double,franja:string,segundos_visualizados:bigint>\n",
      "   +- *(4) Sort [cast(id_contenido#232L as double) ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(cast(id_contenido#232L as double), 3000)\n",
      "         +- *(3) Project [duracion#231L, id_contenido#232L]\n",
      "            +- *(3) Filter isnotnull(id_contenido#232L)\n",
      "               +- *(3) FileScan json [duracion#231L,id_contenido#232L] Batched: false, Format: JSON, Location: InMemoryFileIndex[hdfs://nameservice1/datos/ejercicio_audis/info_contenidos.json], PartitionFilters: [], PushedFilters: [IsNotNull(id_contenido)], ReadSchema: struct<duracion:bigint,id_contenido:bigint>\n"
     ]
    }
   ],
   "source": [
    "merge2.explain() #sortMergeJoin "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como ahora el join es de tipo `SortMergeJoin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.86 s ± 2.6 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit merge2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge2 = audiencias.join(F.broadcast(catalogo), 'id_contenido') #PARA FORZAR EL BROADCAST JOIN AUNQUE ESTE DESACTIVADO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Track\n",
    "\n",
    "Para terminar vamos a ver un último parámetro que pude ser útil para cuando trabajamos en un cluster compartido como es el caso.\n",
    "\n",
    "\n",
    "El parámetro `spark.dynamicAllocation.maxExecutors` limita el número de executors máximos que podemos pedir. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio en clase\n",
    "\n",
    "* ¿Cuántos *executors* podríamos pedir cada alumno si en total somos 22 en clase?\n",
    "* Probar todos la misma configuración y comprobar que podemos trabajar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIGURACION SPARK PARA TRABAJAR EN CLASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "\n",
    "    SparkConf()\n",
    "    .setAppName(u\"[ICAI] Spark Tuning\")\n",
    "    .set(\"spark.executor.memory\", \"7g\")\n",
    "    .set(\"spark.executor.cores\", \"5\")\n",
    "    .set(\"spark.default.parallelism\", 400) #DA IGUAL EL DATA SET QUE VAYA A LEER, LO VA A TRABAJAR CON 3000 PARTICIONES\n",
    "            #para la formula de la particion de un RDD max(.., ..)\n",
    "    .set(\"spark.sql.shuffle.partitions\", 400) #PARA FUNCIONES SHUFFLE \n",
    "    .set(\"spark.dynamicAllocation.maxExecutors\", 2) #el maximo que soy capaz de coger son 49 (si no pones nada los coge todos)\n",
    "\n",
    ")\n",
    "#en el master con 200 nos va a valer siempre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "\n",
    "    SparkSession.builder\n",
    "    .config(conf=conf)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "Logistic-Regression_answers",
  "notebookId": 1355896319138518
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
