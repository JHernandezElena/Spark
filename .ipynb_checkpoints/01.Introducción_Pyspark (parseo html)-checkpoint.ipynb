{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/spark-logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Apache Spark™ is a fast and general engine for large-scale data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Por qué Spark? (una vez más)\n",
    "Hadoop nació (~2005) para procesar grandes cantidades de datos en paralelo. Poco a poco\n",
    "fueron surgiendo nuevas problemáticas que no se podían resolver con el paradigma *MapReduce* y fueron apareciendo\n",
    "nuevos proyectos para solventar estas\n",
    "problemáticas, siendo necesario así una \"jungla\" de programas para un trabajo de big data:\n",
    "\n",
    "&nbsp;    \n",
    "&nbsp;    \n",
    "&nbsp;    \n",
    "\n",
    "\n",
    "![](img/mapreduce_ecosystem.png)\n",
    "\n",
    "\n",
    "&nbsp;    \n",
    "&nbsp;    \n",
    "&nbsp;    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark nace con dos finalidades claras: ser rápido, para ello cambia la manera de trabajar internamente (utiliza memoria, *RDD*,*DAG*...) y unificar\n",
    "bajo un solo proyecto los grandes problemas de datos hasta el momento: Procesamiento en Batch,\n",
    "en *streaming*, *machine learning*, *SQL*...   \n",
    "\n",
    "Además incluye en el mismo proyecto varios lenguajes: Scala, Java, python y R.\n",
    "\n",
    "&nbsp;    \n",
    "&nbsp;     \n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<h1>¡NO SOLO JAVA!</h1>\n",
    "\n",
    "![spark](img/esquema2.png)\n",
    "</center>\n",
    "&nbsp;    \n",
    "&nbsp;    \n",
    "&nbsp;    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoy poy hoy Spark es casi sinonimo de big data y está presente en la mayoria de proyectos siendo la primera opción para el procesamiento masivo de datos. Esto ha hecho que muchas de las \n",
    "aplicaciones ya existentes se hayan hecho compatibles con Spark y que estén surgiendo nuevas\n",
    "enfocadas en trabajar con Spark.\n",
    "\n",
    "&nbsp;    \n",
    "&nbsp;    \n",
    "&nbsp;    \n",
    "\n",
    "\n",
    "![](img/ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"> Python + Spark = PySpark </h1>\n",
    "![](img/pyspark.png)\n",
    "\n",
    "Aunque `spark` está escrito en scala y es el principal lenguaje para trabajar con el. También están soportados otros lenguajes: Java, R y python. \n",
    "\n",
    "Aunque hay soporte para R este todavía es un poco limitado, y si queremos sacar el máximo provecho a spark desde un lenguaje habitual para el análisis de datos, python es la mejor opción a día de hoy.\n",
    "\n",
    "Es más en las últimas versiones de spark se están inplementado nuevas caracteríticas únicas para python como las\n",
    "[vectorized-udfs](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html).\n",
    "\n",
    "\n",
    "&nbsp;    \n",
    "&nbsp;    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde las últimas versiones pyspark se puede usar instalar directamente con [pip](https://pypi.python.org/pypi/pyspark/2.2.0) y podemos consular la documentación [aquí](http://spark.apache.org/docs/latest/api/python/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro cluster podemos acceder por consola con:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pyspark --version\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
    "      /_/\n",
    "                        \n",
    "Type --help for more information.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O a la versión 2 que es la que usaremos con:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pyspark2 --version\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.2.0.cloudera1\n",
    "      /_/\n",
    "                        \n",
    "Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_152\n",
    "Branch HEAD\n",
    "Compiled by user jenkins on 2017-07-13T00:28:58Z\n",
    "Revision 39f5a2b89d29d5d420d88ce15c8c55e2b45aeb2e\n",
    "Url git://github.mtv.cloudera.com/CDH/spark.git\n",
    "Type --help for more information.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conectarnos con `spark` desde el notebook tenemos que configurar la conexión y usar el kernel `Anaconda2`:\n",
    "\n",
    "![](img/kernel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf().setAppName(u\"[ICAI] Intro Pyspark\")) #establece el nombre de la aplicacion de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "\n",
    "    SparkSession.builder\n",
    "    .config(conf=conf)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `spark` es el 'entry point' al framework spark y la que usaremos para interactuar con el cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f3e207ad6a0>\n"
     ]
    }
   ],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=yarn appName=[ICAI] Intro Pyspark>\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count con `pyspark` (LA RUTA POR DEFECTO ES HDFS YA QUE SPARK TRABAJA SOBRE HADOOP Y ES DISTRIBUIDO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme = spark.sparkContext.textFile('/datos/README.md') #lee un fichero de texto plano (LA RUTA ES HDFS)\n",
    "#Tarda poco porque es lazy y no hace nada hasta que hay una accion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.take(1) #lee la primera fila (UNA LISTA DE PYTHON CON UN ELEMENTO DE TIPO STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readme.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Mirar diferencia entre flatmap y map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 47),\n",
       " ('the', 24),\n",
       " ('to', 17),\n",
       " ('Spark', 16),\n",
       " ('for', 12),\n",
       " ('and', 9),\n",
       " ('##', 9),\n",
       " ('a', 8),\n",
       " ('run', 7),\n",
       " ('can', 7)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "\n",
    "    readme #es un rdd: UN RDD ES UNA LISTA DISTRIBUIDA\n",
    "\n",
    "    # Divido cada por espacios (divide por cualquier caracter que sea una separacion)\n",
    "    .flatMap(lambda x: re.split('\\s+',x)) \n",
    "            #si pusieramos solo map nos daria una lista de listas (anidada) por cada fila\n",
    "            #con flatmap todo esta al mismo nivel\n",
    "\n",
    "    # Creo un pair RDD\n",
    "    .map(lambda x: (x,1)) #FUNCION MAP DEVUELVE EL MISMO NUMERO DE ELEMENTOS QUE LA FUNCION DE ORIGEN\n",
    "\n",
    "    # Reduzco por key y sumo los unos para contar\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    # Ordeno de mayor a menor el conteo\n",
    "    .sortBy(lambda x: -x[1]) #ORDENAR POR EL SEGUNDO ELEMENTO X[1] DE MAYOR A MENOR (-)\n",
    "\n",
    ").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De python a spark\n",
    "\n",
    "Podemos pasar variables de python directamente a *RDD's* y viceversa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "colores = np.array(['blue', 'red', 'green', 'yellow', 'brown', 'black'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "muestra = np.random.choice(colores, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(muestra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['blue', 'yellow', 'blue', 'red', 'red', 'yellow', 'red', 'green',\n",
       "       'yellow', 'brown'], dtype='<U6')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muestra[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_muestra = spark.sparkContext.parallelize(muestra, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_muestra.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown',\n",
       " 'red',\n",
       " 'blue',\n",
       " 'brown',\n",
       " 'green',\n",
       " 'green',\n",
       " 'black',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'yellow']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_muestra.take(10) #Siempre que haces un take devuekve una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('green', 166061),\n",
       " ('brown', 166697),\n",
       " ('black', 167208),\n",
       " ('yellow', 166636),\n",
       " ('red', 166811),\n",
       " ('blue', 166587)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "\n",
    "    rdd_muestra\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arriba es como hacerlo con pyspark, abajo solo con python\n",
    "Usamos pyspark cuando tenemos MUCHOS datos y no podemos trabajar en nuestro ordenador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['black', 'blue', 'brown', 'green', 'red', 'yellow'], dtype='<U6'),\n",
       " array([167208, 166587, 166697, 166061, 166811, 166636]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(muestra, return_counts=True) #No ponemos collect porque muestra ya es de python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y si los datos no caben en memoría?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gran_muestra = (\n",
    "\n",
    "    spark.sparkContext\n",
    "    # Usamos range como si fuera un bucle\n",
    "    .range(10, numSlices=200)\n",
    "    .flatMap(lambda _: np.random.choice(colores, 1000000))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brown',\n",
       " 'red',\n",
       " 'blue',\n",
       " 'green',\n",
       " 'red',\n",
       " 'yellow',\n",
       " 'brown',\n",
       " 'blue',\n",
       " 'red',\n",
       " 'red']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gran_muestra.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00E+07\n"
     ]
    }
   ],
   "source": [
    "print('%.2E' % (10 * 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00E+07\n"
     ]
    }
   ],
   "source": [
    "n = gran_muestra.count()\n",
    "n\n",
    "print('%.2E' % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('green', 1668829),\n",
       " ('brown', 1667065),\n",
       " ('black', 1665275),\n",
       " ('yellow', 1666725),\n",
       " ('blue', 1666090),\n",
       " ('red', 1666016)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "\n",
    "    gran_muestra\n",
    "    .map(lambda x: (x, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkred'> CUIDADO:\n",
    "\n",
    "Al usar `collect` se recoge todo el *RDD* en el driver, así que hay que estar seguro de que el tamaño sea pequeño o tendremos problemas de memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\n",
    "np.unique(gran_muestra.collect(),return_counts=True)\n",
    "`\n",
    "*Ponemos collect porque gran muestra es un RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer ficheros del HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxrwxrwx   3 jayuso jayuso    2772143 2017-12-02 12:11 /datos/diamonds.csv\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /datos/diamonds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n",
      "0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n",
      "0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n",
      "0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n",
      "0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n",
      "0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n",
      "0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n",
      "0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n",
      "0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n",
      "0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n",
      "text: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -text /datos/diamonds.csv | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds = spark.sparkContext.textFile('/datos/diamonds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"',\n",
       " '0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43',\n",
       " '0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31',\n",
       " '0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31',\n",
       " '0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63',\n",
       " '0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75',\n",
       " '0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48',\n",
       " '0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47',\n",
       " '0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53',\n",
       " '0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtrado = (\n",
    "\n",
    "    diamonds\n",
    "    .map(lambda x: x.split(','))\n",
    "    .filter(lambda x: 'Fair' in x[1] and 'SI2' in x[3])\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtrado.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0.86',\n",
       "  '\"Fair\"',\n",
       "  '\"E\"',\n",
       "  '\"SI2\"',\n",
       "  '55.1',\n",
       "  '69',\n",
       "  '2757',\n",
       "  '6.45',\n",
       "  '6.33',\n",
       "  '3.52'],\n",
       " ['0.96',\n",
       "  '\"Fair\"',\n",
       "  '\"F\"',\n",
       "  '\"SI2\"',\n",
       "  '66.3',\n",
       "  '62',\n",
       "  '2759',\n",
       "  '6.27',\n",
       "  '5.95',\n",
       "  '4.07']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtrado.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones y Acciones\n",
    "\n",
    "La API de Pyspark es muy parecida al core en *scala*. También tenemos transformaciones y acciones:    \n",
    "&nbsp;   \n",
    "\n",
    "<center>\n",
    "![](img/RDD_Operations.png)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations\n",
    "* https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "peliculas = [\n",
    "    'http://www.imdb.com/title/tt0071562',\n",
    "    'http://www.imdb.com/title/tt0110912',\n",
    "    'http://www.imdb.com/title/tt0050083',\n",
    "    'http://www.imdb.com/title/tt0108052',\n",
    "    'http://www.imdb.com/title/tt0468569',\n",
    "    'http://www.imdb.com/title/tt0068646',\n",
    "    'http://www.imdb.com/title/tt0167260',\n",
    "    'http://www.imdb.com/title/tt0060196',\n",
    "    'http://www.imdb.com/title/tt0137523',\n",
    "    'http://www.imdb.com/title/tt0111161',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsear_html(texto):\n",
    "    soup = BeautifulSoup(texto,'lxml')\n",
    "    item = dict()\n",
    "    item['titulo'] = soup.find(\"h1\").find(text=True).replace(u'\\xa0',' ').strip()\n",
    "    item['ratingvalue'] = float(soup.select_one('[itemprop=\"ratingValue\"]').text)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "descargas = (\n",
    "    \n",
    "    spark.sparkContext\n",
    "    .parallelize(peliculas)\n",
    "    .map(lambda x: requests.get(x).content)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descargas.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseados = descargas.map(parsear_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'titulo': 'El padrino: Parte II', 'ratingvalue': 9.0},\n",
       " {'titulo': 'Pulp Fiction', 'ratingvalue': 8.9},\n",
       " {'titulo': '12 hombres sin piedad', 'ratingvalue': 8.9},\n",
       " {'titulo': 'La lista de Schindler', 'ratingvalue': 8.9},\n",
       " {'titulo': 'El caballero oscuro', 'ratingvalue': 9.0},\n",
       " {'titulo': 'El padrino', 'ratingvalue': 9.2},\n",
       " {'titulo': 'El señor de los anillos: El retorno del rey', 'ratingvalue': 8.9},\n",
       " {'titulo': 'El bueno, el feo y el malo', 'ratingvalue': 8.8},\n",
       " {'titulo': 'El club de la lucha', 'ratingvalue': 8.8},\n",
       " {'titulo': 'Cadena perpetua', 'ratingvalue': 9.3}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parseados.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache\n",
    "\n",
    "Podemos cachear un *RDD*, para no tener que recalcuarlo cada vez. Muy útil cuando estamos explorando los datos o tenemos que hacer dos acciones distintas sobre el mismo *RDD*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24 s ± 159 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "descargas.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.29 s ± 174 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "parseados.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[49] at RDD at PythonRDD.scala:52"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descargas.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 ms ± 44.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "descargas.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseados = descargas.map(parsear_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239 ms ± 61.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "parseados.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descargas.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[49] at RDD at PythonRDD.scala:52"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descargas.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descargas.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Al finalizar, siempre hay que cerrar la conexión de spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "Introduction to Apache Spark on Databricks (2)",
  "notebookId": 687660855473850
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
