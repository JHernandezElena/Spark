{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"> Extendiendo Spark </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/spark-hub.jpg)\n",
    "\n",
    "\n",
    "Cada vez la comunidad de Spark es más grande y podemos hacer más cosas, vamos a ver algunos ejemplos de qué más podemos hacer con Spark:\n",
    "\n",
    "Empezamos iniciando la sesión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "\n",
    "    SparkConf()\n",
    "    .setAppName(u\"[ICAI] Extendiendo Spark\")\n",
    "    .set(\"spark.executor.memory\",\"8g\")\n",
    "    .set(\"spark.executor.cores\",\"5\")\n",
    "    .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.11:2.2.1\") #para bajarse el jar y las dependecias \n",
    "                    #de mongodb\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "\n",
    "    SparkSession.builder\n",
    "    .config(conf=conf)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF: *User Defined Functions*\n",
    "\n",
    "Con las `UDF` podemos extender las funcionalidad de spark *DataFrame* igual que haciamos con los `RDD` y utilizar cualquier función de python. Veamos un ejemplo:\n",
    "*Penalizan en tiempo. Si exsite el F.funcion hacerlo mejor asi!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiencias = spark.read.load('/datos/ejercicio_audis/audiencias_large.parquet').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuantiles = (\n",
    "\n",
    "    audiencias\n",
    "    .select(F.expr(\"\"\" percentile_approx(segundos_visualizados,array(0,.25,.5,.75,1)) as cuantiles \"\"\"))\n",
    "    .first()[0]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 183, 751, 2827, 83042]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findInterval(x):\n",
    "    return bisect.bisect(cuantiles,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `findInterval` nos indica en qué intervalo se encuentra el valor de `x` respecto a los cuantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findInterval(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findInterval(4500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con `F.udf` convertimos la función `findInterval` para trabajar con spark, tenemos que definir el tipo que va a devolver (en este caso `integer`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "findInterval_udf = F.udf(findInterval,T.IntegerType()) #T.tipo que voy a devolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiencias_cuantiles = (\n",
    "    \n",
    "    audiencias\n",
    "    .select(\n",
    "        findInterval_udf('segundos_visualizados').alias('cuantil')\n",
    "    )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|cuantil|\n",
      "+-------+\n",
      "|      4|\n",
      "|      3|\n",
      "|      1|\n",
      "|      1|\n",
      "|      4|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audiencias_cuantiles.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|          cuantil|\n",
      "+-------+-----------------+\n",
      "|  count|         51191302|\n",
      "|   mean|2.499854877689964|\n",
      "| stddev|1.118069930091352|\n",
      "|    min|                1|\n",
      "|    max|                5|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "audiencias_cuantiles.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MAS EFICIENTE QUE CON UNA UDF\n",
    "prueba1 = (\n",
    "\n",
    "    audiencias\n",
    "    .select(\n",
    "        F.lower(F.split('franja','_')[1]).alias('nuevo')\n",
    "    )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba2 = (\n",
    "\n",
    "    audiencias\n",
    "    .select(\n",
    "        ( F.udf(lambda x: (x.split('_')[1]).lower() )('franja') ).alias('nuevo')\n",
    "    )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    nuevo|\n",
      "+---------+\n",
      "|   manana|\n",
      "|primetime|\n",
      "|madrugada|\n",
      "|    tarde|\n",
      "|    noche|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prueba1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    nuevo|\n",
      "+---------+\n",
      "|   manana|\n",
      "|primetime|\n",
      "|madrugada|\n",
      "|    tarde|\n",
      "|    noche|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prueba2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las `UDF` son muy versátiles y nos abre un gran mundo de posibilidades pero son más lentas que usar las funciones de spark así que siempre que podamos usaremos estas últimas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas UDF\n",
    "\n",
    "Las [Pandas UDF](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html) fueron introducidas en Spark 2.3. Tienen la misma idea que las UDF pero con mayor *performance*:\n",
    "\n",
    "![](./img/pandas_udf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas funciones consiguen mayor velocidad gracias al proyecto [Apache Arrow](https://arrow.apache.org/):\n",
    "\n",
    "\n",
    "\n",
    "![](img/apache_arrow.png)\n",
    "\n",
    "*Manera de guardar datos de forma columnal. Escribe por bloques en tipo Arrow\n",
    "*Recall: pandas por debajo es numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = audiencias.select('franja').limit(20).toPandas().franja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(T.StringType()) #es lo mismo que pasar la x por el pandaUDF \n",
    "def pandas_tratar(x):\n",
    "    return x.str.split('_',1).str[0].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba5 = (\n",
    "\n",
    "    audiencias\n",
    "    .select(\n",
    "        pandas_tratar('franja').alias('nuevo')\n",
    "    )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|nuevo|\n",
      "+-----+\n",
      "|finde|\n",
      "|finde|\n",
      "|finde|\n",
      "|finde|\n",
      "|entre|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prueba5.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programación más compleja usando python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "listas = spark.read.load('/datos/listas/listas.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = spark.read.load('/datos/listas/item.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|categorias|count|\n",
      "+----------+-----+\n",
      "|        16| 2141|\n",
      "|       128|  721|\n",
      "|        80|  286|\n",
      "|         0|  274|\n",
      "|       112|  239|\n",
      "|        96|   39|\n",
      "|        64|   30|\n",
      "|        48|    1|\n",
      "|      null|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#para vemos cuantas cosas hay por categorias\n",
    "(\n",
    "\n",
    "    items\n",
    "    .select(F.explode('categorias').alias('categorias'))\n",
    "    .groupBy('categorias')\n",
    "    .count()\n",
    "    .orderBy(F.desc('count'))\n",
    "\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 1**: Queremos obtener los 20 primeros items para la categoría 16 para cada usuario y tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "listas_16 = (\n",
    "\n",
    "    listas\n",
    "    .join(\n",
    "        items\n",
    "        .filter(F.array_contains('categorias',16)), #filra las filas donde items tenga la categoria 16\n",
    "        'id_item', #hacemos join por id_item\n",
    "        'leftsemi' #tipo de join?\n",
    "    )\n",
    "    .withColumn(\n",
    "        'rnk',\n",
    "        F.row_number() #row_number no da repeticiones\n",
    "        .over(\n",
    "            Window\n",
    "            .partitionBy('id_user','tipo')\n",
    "            .orderBy(F.desc('rating'))\n",
    "        )\n",
    "    )\n",
    "    .filter('rnk<=20') #cogemos los ranks<=20\n",
    "    .drop('rating') #me cargo la variable rating porque ya no la necesita\n",
    "    .withColumn('categoria',F.lit(16)) #F.lit es una columna que siempre vale 16\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+---+---------+\n",
      "|id_item|id_user|tipo|rnk|categoria|\n",
      "+-------+-------+----+---+---------+\n",
      "|   11.0|   36.0|azul|  1|       16|\n",
      "|  101.0|   36.0|azul|  2|       16|\n",
      "|  158.0|   36.0|azul|  3|       16|\n",
      "|    7.0|   36.0|azul|  4|       16|\n",
      "|   43.0|   36.0|azul|  5|       16|\n",
      "|  344.0|   36.0|azul|  6|       16|\n",
      "|  274.0|   36.0|azul|  7|       16|\n",
      "|  115.0|   36.0|azul|  8|       16|\n",
      "|   83.0|   36.0|azul|  9|       16|\n",
      "|   14.0|   36.0|azul| 10|       16|\n",
      "|   15.0|   36.0|azul| 11|       16|\n",
      "|   30.0|   36.0|azul| 12|       16|\n",
      "|  258.0|   36.0|azul| 13|       16|\n",
      "|    1.0|   36.0|azul| 14|       16|\n",
      "|  250.0|   36.0|azul| 15|       16|\n",
      "|  123.0|   36.0|azul| 16|       16|\n",
      "| 1789.0|   36.0|azul| 17|       16|\n",
      "|  239.0|   36.0|azul| 18|       16|\n",
      "|  110.0|   36.0|azul| 19|       16|\n",
      "|  131.0|   36.0|azul| 20|       16|\n",
      "+-------+-------+----+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listas_16.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             count|\n",
      "+-------+------------------+\n",
      "|  count|            161900|\n",
      "|   mean| 19.98575046324892|\n",
      "| stddev|0.3948110172107952|\n",
      "|    min|                 1|\n",
      "|    max|                20|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listas_16.groupBy('id_user','tipo').count().describe('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 2**: Hacemos lo mismo para la categoría 112."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "listas_112 = (\n",
    "\n",
    "    listas\n",
    "    .join(\n",
    "        items\n",
    "        .filter(F.array_contains('categorias',112)),\n",
    "        'id_item',\n",
    "        'leftsemi'\n",
    "    )\n",
    "    .withColumn(\n",
    "        'rnk',\n",
    "        F.row_number()\n",
    "        .over(\n",
    "            Window\n",
    "            .partitionBy('id_user','tipo')\n",
    "            .orderBy(F.desc('rating'))\n",
    "        )\n",
    "    )\n",
    "    .filter('rnk<=20')\n",
    "    .drop('rating')\n",
    "    .withColumn('categoria',F.lit(112))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             count|\n",
      "+-------+------------------+\n",
      "|  count|            160443|\n",
      "|   mean|19.766272134028906|\n",
      "| stddev| 1.887585046190908|\n",
      "|    min|                 1|\n",
      "|    max|                20|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listas_112.groupBy('id_user','tipo').count().describe('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 3**: Unir las listas `.union` coge un dataframe y lo une con otro anadiendo las filas (tiene que tener el mismo numero de columnas y los mismos tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "listas_unidas = (\n",
    "\n",
    "    listas_16\n",
    "    .union(listas_112)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+---+---------+\n",
      "|id_item|id_user|tipo|rnk|categoria|\n",
      "+-------+-------+----+---+---------+\n",
      "|   11.0|   36.0|azul|  1|       16|\n",
      "|  101.0|   36.0|azul|  2|       16|\n",
      "|  158.0|   36.0|azul|  3|       16|\n",
      "|    7.0|   36.0|azul|  4|       16|\n",
      "|   43.0|   36.0|azul|  5|       16|\n",
      "|  344.0|   36.0|azul|  6|       16|\n",
      "|  274.0|   36.0|azul|  7|       16|\n",
      "|  115.0|   36.0|azul|  8|       16|\n",
      "|   83.0|   36.0|azul|  9|       16|\n",
      "|   14.0|   36.0|azul| 10|       16|\n",
      "|   15.0|   36.0|azul| 11|       16|\n",
      "|   30.0|   36.0|azul| 12|       16|\n",
      "|  258.0|   36.0|azul| 13|       16|\n",
      "|    1.0|   36.0|azul| 14|       16|\n",
      "|  250.0|   36.0|azul| 15|       16|\n",
      "|  123.0|   36.0|azul| 16|       16|\n",
      "| 1789.0|   36.0|azul| 17|       16|\n",
      "|  239.0|   36.0|azul| 18|       16|\n",
      "|  110.0|   36.0|azul| 19|       16|\n",
      "|  131.0|   36.0|azul| 20|       16|\n",
      "+-------+-------+----+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listas_unidas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paso 4**: Queremos construir un DF como `listas_unidas` para unas categorías dadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiero = [16, 80, 112]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la funcion que hemos usado para la categoria 16 y la 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_lista(x):\n",
    "\n",
    "    return (\n",
    "        listas\n",
    "        .join(\n",
    "            items\n",
    "            .filter(F.array_contains('categorias',x)),\n",
    "            'id_item',\n",
    "            'leftsemi'\n",
    "        )\n",
    "        .withColumn(\n",
    "            'rnk',\n",
    "            F.row_number()\n",
    "            .over(\n",
    "                Window\n",
    "                .partitionBy('id_user','tipo')\n",
    "                .orderBy(F.desc('rating'))\n",
    "            )\n",
    "        )\n",
    "        .filter('rnk<=20')\n",
    "        .drop('rating')\n",
    "        .withColumn('categoria',F.lit(x))\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el `map` de python para gener una lista de `DF` de spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f357206fb38>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(generar_lista,quiero) #hace lo mismo que lo de abajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[generar_lista(i) for i in quiero]\n",
    "#generara una lista de 3 dataframes de pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con `reduce` unimos todos los `DFs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_final = reduce(lambda x,y: x.union(y), map(generar_lista,quiero)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id_item: double, id_user: double, tipo: string, rnk: int, categoria: int]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9570210"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+---+---------+\n",
      "|id_item|id_user|tipo|rnk|categoria|\n",
      "+-------+-------+----+---+---------+\n",
      "|   11.0|   36.0|azul|  1|       16|\n",
      "|  101.0|   36.0|azul|  2|       16|\n",
      "|  158.0|   36.0|azul|  3|       16|\n",
      "|    7.0|   36.0|azul|  4|       16|\n",
      "|   43.0|   36.0|azul|  5|       16|\n",
      "|  344.0|   36.0|azul|  6|       16|\n",
      "|  274.0|   36.0|azul|  7|       16|\n",
      "|  115.0|   36.0|azul|  8|       16|\n",
      "|   83.0|   36.0|azul|  9|       16|\n",
      "|   14.0|   36.0|azul| 10|       16|\n",
      "|   15.0|   36.0|azul| 11|       16|\n",
      "|   30.0|   36.0|azul| 12|       16|\n",
      "|  258.0|   36.0|azul| 13|       16|\n",
      "|    1.0|   36.0|azul| 14|       16|\n",
      "|  250.0|   36.0|azul| 15|       16|\n",
      "|  123.0|   36.0|azul| 16|       16|\n",
      "| 1789.0|   36.0|azul| 17|       16|\n",
      "|  239.0|   36.0|azul| 18|       16|\n",
      "|  110.0|   36.0|azul| 19|       16|\n",
      "|  131.0|   36.0|azul| 20|       16|\n",
      "+-------+-------+----+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lista_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             count|\n",
      "+-------+------------------+\n",
      "|  count|            482918|\n",
      "|   mean|19.817463834439803|\n",
      "| stddev|1.6688515269853819|\n",
      "|    min|                 1|\n",
      "|    max|                20|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lista_final.groupBy('id_user','tipo','categoria').count().describe('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Packages\n",
    "\n",
    "En la web https://spark-packages.org/, podemos encontrar multitud de paquetes para ampliar el uso de spark. Veamos un ejemplo para conectar a [MongoDB](https://www.mongodb.com/) una base de datos de tipo NoSQL ampliamente utilizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/mongo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/spark-connector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la configuración de spark hemos cargado este paquete de la siguiente manera:\n",
    "\n",
    "\n",
    "```python\n",
    ".set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.11:2.2.1\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leer\n",
    "De este modo podemos leer un `DF` desde MongoDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pelis = (\n",
    "\n",
    "    spark.read\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "    .option(\"uri\",\"mongodb://edge01.bigdata.alumnos.upcont.es/imdb.pelis\")\n",
    "    .load()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- ratingvalue: double (nullable = true)\n",
      " |-- titulo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pelis.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>ratingvalue</th>\n",
       "      <th>titulo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(5dda6d63d572d75a3c424b0a,)</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Cadena perpetua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(5dda6d63d572d75a3c424b06,)</td>\n",
       "      <td>9.2</td>\n",
       "      <td>El padrino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(5dda6d63d572d75a3c424b01,)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>El padrino: Parte II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(5dda6d63d572d75a3c424b05,)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>El caballero oscuro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(5dda6d63d572d75a3c424b02,)</td>\n",
       "      <td>8.9</td>\n",
       "      <td>Pulp Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(5dda6d63d572d75a3c424b03,)</td>\n",
       "      <td>8.9</td>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(5dda6d63d572d75a3c424b08,)</td>\n",
       "      <td>8.9</td>\n",
       "      <td>El bueno, el feo y el malo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(5dda6d63d572d75a3c424b07,)</td>\n",
       "      <td>8.9</td>\n",
       "      <td>El señor de los anillos: El retorno del rey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(5dda6d63d572d75a3c424b04,)</td>\n",
       "      <td>8.9</td>\n",
       "      <td>La lista de Schindler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(5dda6d63d572d75a3c424b09,)</td>\n",
       "      <td>8.8</td>\n",
       "      <td>El club de la lucha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           _id  ratingvalue  \\\n",
       "0  (5dda6d63d572d75a3c424b0a,)          9.3   \n",
       "1  (5dda6d63d572d75a3c424b06,)          9.2   \n",
       "2  (5dda6d63d572d75a3c424b01,)          9.0   \n",
       "3  (5dda6d63d572d75a3c424b05,)          9.0   \n",
       "4  (5dda6d63d572d75a3c424b02,)          8.9   \n",
       "5  (5dda6d63d572d75a3c424b03,)          8.9   \n",
       "6  (5dda6d63d572d75a3c424b08,)          8.9   \n",
       "7  (5dda6d63d572d75a3c424b07,)          8.9   \n",
       "8  (5dda6d63d572d75a3c424b04,)          8.9   \n",
       "9  (5dda6d63d572d75a3c424b09,)          8.8   \n",
       "\n",
       "                                        titulo  \n",
       "0                              Cadena perpetua  \n",
       "1                                   El padrino  \n",
       "2                         El padrino: Parte II  \n",
       "3                          El caballero oscuro  \n",
       "4                                 Pulp Fiction  \n",
       "5                        12 hombres sin piedad  \n",
       "6                   El bueno, el feo y el malo  \n",
       "7  El señor de los anillos: El retorno del rey  \n",
       "8                        La lista de Schindler  \n",
       "9                          El club de la lucha  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pelis.orderBy(F.desc('ratingvalue')).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escribir\n",
    "Del mismo modo podemos escribir en el mongodb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogo = spark.read.json('/datos/catalogo.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|duracion|id_contenido|\n",
      "+--------+------------+\n",
      "|    7014|         171|\n",
      "|    9177|        8599|\n",
      "|     869|        7754|\n",
      "|    7223|          14|\n",
      "|    3600|        8418|\n",
      "|    2324|        8004|\n",
      "|    6671|        9852|\n",
      "|    3848|        6577|\n",
      "|    1245|       10412|\n",
      "|     410|        9668|\n",
      "|    4517|       10088|\n",
      "|    7200|        2083|\n",
      "|    1791|        8079|\n",
      "|    5918|        6181|\n",
      "|    5400|        3432|\n",
      "|    7548|          28|\n",
      "|    5457|        2794|\n",
      "|    6902|        8903|\n",
      "|    5360|        5436|\n",
      "|    1852|        8104|\n",
      "+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catalogo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    catalogo\n",
    "    .write.format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"uri\",\"mongodb://edge01.bigdata.alumnos.upcont.es\")\n",
    "    .option(\"database\",os.environ.get('USER'))\n",
    "    .option(\"collection\", \"catalogo\")\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "Introduction to Apache Spark on Databricks (2)",
  "notebookId": 687660855473850
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
